{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NCwQQFkU3v5"
   },
   "source": [
    "### 2.   Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAIO_Y5Z9_Ay"
   },
   "source": [
    "**[2.1]** Import the torch and torchvision packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2VRE9JYD9_Kk"
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbRDgod3_3Yv"
   },
   "source": [
    "**[2.2]** Create a variable called `download` containing the value `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gcaDN6V3_3q9"
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "download = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-Zy6Oq8pkuB"
   },
   "source": [
    "**[2.3]** Define a transformation pipeline that will convert the images into tensors and normalise them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q1iETWjDftMg"
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLyMcoNCsx2k"
   },
   "source": [
    "**[2.4]** Instantiate a torchvision.datasets.MNIST() for the training set, downlows it into `/data/raw/` folder and perform the transformation defined earlier. Save the results in a variable called `train_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "xvnbhiPhs0ZP",
    "outputId": "e53ecd49-2f83-4391-8d44-6a02fdcc13df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "train_data = torchvision.datasets.MNIST('../data/raw/', train=True, download=download, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQgeYjQDs12m"
   },
   "source": [
    "**[2.5]** Instantiate a torchvision.datasets.MNIST() for the testing set, downlows it into `/data/raw/` folder and perform the transformation defined earlier. Save the results in a variable called `test_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dg_89DlAs1_w",
    "outputId": "7b5cdeac-b62b-4a49-dd1f-db9dbb336fd0"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "test_data = torchvision.datasets.MNIST('../data/raw/', train=False, download=download, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miQ6SiKlscLx"
   },
   "source": [
    "### 3. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtuF1V6ctwn-"
   },
   "source": [
    "**[3.1]** Create 2 variables called `batch_size_train` and `batch_size_test` that will respectively take the values 64 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HrXR7NCLtwxB"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "batch_size_train = 64\n",
    "batch_size_test = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "femTzeCBN0Wf"
   },
   "source": [
    "**[3.2]** Import DataLoader from torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IgrFk1bhN0qO"
   },
   "outputs": [],
   "source": [
    "# Solution:\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1RrTCBRDybQ"
   },
   "source": [
    "**[3.3]** Instantiate a `torch.utils.data.DataLoader()` on the training data, with the relevant batch size and with shuffle. Save the reults in a variable called `train_loader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aezRs9S3Dyl0"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pKaCxasERAt"
   },
   "source": [
    "**[3.4]** Instantiate a `torch.utils.data.DataLoader()` on the testing data, with the relevant batch size and with shuffle. Save the reults in a variable called `test_loader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yDwCmIXvERJO"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-l0Hkri1FVrv"
   },
   "source": [
    "**[3.5]** Create a generator on the test data loader and extract the first observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "x_yCjMqgFV1u"
   },
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2v1vLMWTofp"
   },
   "source": [
    "**[3.6]** Print the dimensions of the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dfUpbRYSTopS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lt8YYcHuTsDs"
   },
   "source": [
    "**[3.7]** Import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "z6isEtGBTsNz"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guu_QM_KN8Aw"
   },
   "source": [
    "**[3.8]** Print the first image with its corresponding target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Gv7PFYRONv0o"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ground Truth: 9')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(example_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.title(\"Ground Truth: {}\".format(example_targets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8MNBrC4Zgz6"
   },
   "source": [
    "### 4. Define Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Lt1lE8lJ9zS"
   },
   "source": [
    "**[4.1]** Import torch.nn as n, torch.nn.functional as F and torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2xufkK8VJ99s"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFCSRIyMZuAR"
   },
   "source": [
    "**[4.2]** Create a class called `PytorchCNN` that inherits from `nn.Module` with:\n",
    "- attributes:\n",
    "    - `conv1`: fully-connected layer with 128 filters of size 3\n",
    "    - `conv2`: fully-connected layer with 64 filters of size 3\n",
    "    - `fc1`: fully-connected layer with 128 neurons\n",
    "    - `fc2`: fully-connected layer with 10 neurons\n",
    "    - `softmax`: Softmax activation function\n",
    "- methods:\n",
    "    - `forward()` with `inputs` as input parameter and will sequentially add the 2 convolution layers with relu and max pool of size 2 followed the 2 full-connected layers respectively with relu and softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Eejq-COGZhCP"
   },
   "outputs": [],
   "source": [
    "# Solution:\n",
    "class PytorchCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PytorchCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJNZfvA4dJ9X"
   },
   "source": [
    "**[4.3]** Instantiate a PytorchCNN and save it into a variable called `model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fev4FWAYdU1G"
   },
   "outputs": [],
   "source": [
    "# Solution:\n",
    "model = PytorchCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRLIZeci7cfW"
   },
   "source": [
    "**[4.4]** Import the `get_device` function from src.models.pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Z1Jt8WX57cqn"
   },
   "outputs": [],
   "source": [
    "# Solution:\n",
    "from src.models.pytorch import get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyV4P9b_LML4"
   },
   "source": [
    "**[4.5]** Get the device available and set to the model to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "O6l_Y_dHLMUR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PytorchCNN(\n",
       "  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1600, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution:\n",
    "device = get_device()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUEbyrm2ZzhL"
   },
   "source": [
    "### 5. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faMubeDzZzuX"
   },
   "source": [
    "**[5.1]** Import train_classification and test_classification from src.models.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vBSoR7LTZz3-"
   },
   "outputs": [],
   "source": [
    "# Solution:\n",
    "from src.models.pytorch import train_classification, test_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORcJNo4ygaRa"
   },
   "source": [
    "**[5.2]** Instantiate a `nn.CrossEntropyLoss()` and save it into a variable called `criterion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QkXUqKMcgaZN"
   },
   "outputs": [],
   "source": [
    "# Solution:\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBxCIn10ZDAF"
   },
   "source": [
    "**[5.3]** Instantiate a torch.optim.Adam() optimizer with the model's parameters and 0.001 as learning rate and save it into a variable called optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "c6oNYKJEZDM4"
   },
   "outputs": [],
   "source": [
    "# Solution:\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR22BA8dZnKz"
   },
   "source": [
    "**[5.4]** Create 2 variables called `N_EPOCHS` and `BATCH_SIZE` that will respectively take the values 5 and 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "nmw1xla2ZnYT"
   },
   "outputs": [],
   "source": [
    "# Solution:\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekaUYYjqgfcF"
   },
   "source": [
    "**[5.5]** Create a for loop that will iterate through the specified number of epochs and will train the model with the training set and assess the performance on the validation set and print their scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "h8Jmfhk0MQ0i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t(train)\t|\tLoss: 0.0499\t|\tAcc: 86.6%\n",
      "\t(valid)\t|\tLoss: 0.0496\t|\tAcc: 87.4%\n",
      "Epoch: 1\n",
      "\t(train)\t|\tLoss: 0.0477\t|\tAcc: 93.3%\n",
      "\t(valid)\t|\tLoss: 0.0463\t|\tAcc: 98.2%\n",
      "Epoch: 2\n",
      "\t(train)\t|\tLoss: 0.0463\t|\tAcc: 97.9%\n",
      "\t(valid)\t|\tLoss: 0.0463\t|\tAcc: 98.3%\n",
      "Epoch: 3\n",
      "\t(train)\t|\tLoss: 0.0462\t|\tAcc: 98.1%\n",
      "\t(valid)\t|\tLoss: 0.0464\t|\tAcc: 97.9%\n",
      "Epoch: 4\n",
      "\t(train)\t|\tLoss: 0.0462\t|\tAcc: 98.3%\n",
      "\t(valid)\t|\tLoss: 0.0461\t|\tAcc: 98.7%\n"
     ]
    }
   ],
   "source": [
    "# Solution:\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_classification(train_data, model=model, criterion=criterion, optimizer=optimizer, batch_size=BATCH_SIZE, device=device)\n",
    "    valid_loss, valid_acc = test_classification(test_data, model=model, criterion=criterion, batch_size=BATCH_SIZE, device=device)\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t(train)\\t|\\tLoss: {train_loss:.4f}\\t|\\tAcc: {train_acc * 100:.1f}%')\n",
    "    print(f'\\t(valid)\\t|\\tLoss: {valid_loss:.4f}\\t|\\tAcc: {valid_acc * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE8fVHin92-6"
   },
   "source": [
    "**[5.6]** Save the model into the `models` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oludTfN193I-"
   },
   "outputs": [],
   "source": [
    "# Solution:\n",
    "torch.save(model, \"../models/pytorch_mnist_cnn.pt\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Advanced DS for Innovation - Lab 6 - Exercise 1 - Solutions",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
